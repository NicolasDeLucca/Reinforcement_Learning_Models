# -*- coding: utf-8 -*-
"""Acrobot_Domain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0ykkRz03OmQV6JRfWCLp4TXgwoJyNKA

**Autores**

*   [202746] Nicolas, De Lucca
*   [285003] Franco, Bottero
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import pickle
from gym.envs.classic_control.acrobot import AcrobotEnv
from random import randint

"""#### PEAS Context"""

# Environment
class AcrobotEnvExtended(AcrobotEnv):
    def __init__(self, *args, **kwargs):
        super(AcrobotEnvExtended, self).__init__(*args, **kwargs)
        self.max_steps = 700
        self.actual_steps = 0

    def step(self, action):
        self.actual_steps += 1
        obs, reward, done, _, _ = super().step(action)
        done = done or self.actual_steps >= self.max_steps
        return obs, reward, done

    def reset(self, *args, **kwargs):
        self.actual_steps = 0
        return super().reset(*args, **kwargs)

# Constants
cost1_space = np.linspace(-1, 1, 5)
sint1_space = np.linspace(-1, 1, 5)
cost2_space = np.linspace(-1, 1, 10)
sint2_space = np.linspace(-1, 1, 10)
velt1_space = np.linspace(-12.57, 12.57, 30)
velt2_space = np.linspace(-28.27, 28.27, 20)

def get_state(obs):
    c1,s1,c2,s2,vt1,vt2 = obs[:6]
    c1_bin = np.digitize(c1, cost1_space)
    s1_bin = np.digitize(s1, sint1_space)
    c2_bin = np.digitize(c2, cost2_space)
    s2_bin = np.digitize(s2, sint2_space)
    vt1_bin = np.digitize(vt1, velt1_space)
    vt2_bin = np.digitize(vt2, velt2_space)
    return c1_bin, s1_bin, c2_bin, s2_bin, vt1_bin, vt2_bin

# Performance
def optimal_policy(state, Q):
    action = np.argmax(Q[state])
    return action

"""#### Q-Learning Agent"""

# Greedy Policy
def epsilon_greedy_policy(env, state, Q, epsilon):
    explore = np.random.binomial(1, epsilon)
    if explore:
        action = env.action_space.sample()
    else:
        action = np.argmax(Q[state])
    return action


class QLearningAgent:
    def __init__(self, env, alpha, gamma, epsilon):
        actions = list(range(env.action_space.n))
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = np.zeros((len(cost1_space)+1, len(sint1_space)+1, len(cost2_space)+1,
              len(sint2_space)+1, len(velt1_space)+1, len(velt2_space)+1, len(actions)))

    def discretize_state(self, obs):
        return get_state(obs)

    def select_action(self, state):
        return epsilon_greedy_policy(self.env, state, self.Q, self.epsilon)

    def update_q_value(self, state, action, reward, next_state):
        best_next_action = optimal_policy(state, self.Q)
        current_q = self.Q[state][action]
        next_q = self.Q[next_state][best_next_action]
        new_q = current_q + self.alpha * (reward + self.gamma * next_q - current_q)
        self.Q[state][action] = new_q

    def train(self, num_episodes):
         for episode in range(1, num_episodes + 1):
              obs = self.env.reset()
              state = self.discretize_state(obs)
              total_reward = 0
              done = False
              while not done:
                  action = self.select_action(state)
                  step_result = self.env.step(action)
                  next_obs, reward, done = step_result[:3]
                  next_state = self.discretize_state(next_obs)
                  self.update_q_value(state, action, reward, next_state)
                  state = next_state
                  total_reward += reward

    def save(self, filename='q_learning_agent.pkl'):
        with open(filename, 'wb') as file:
            pickle.dump(self, file)

"""#### Simulation

"""

def simulate(env: AcrobotEnvExtended, q_learning_agent: QLearningAgent):
    obs = env.reset()
    print(f"Observation: {obs}\n")
    done = False
    episode_reward = 0
    while not done:
        state = q_learning_agent.discretize_state(obs)
        action = q_learning_agent.select_action(state)
        step_result = env.step(action)
        obs, reward, done = step_result[:3]
        episode_reward += reward
        print('->', f"State: {state}", f"Action: {action}", f"Reward: {reward}", f"Observation: {obs}\n")
    print(f"Episode reward: {episode_reward}\n")

"""#### Policy Evaluation"""

def evaluate_policy(num_episodes, env : AcrobotEnvExtended, q_learning_agent: QLearningAgent):
    total_reward = 0
    for _ in range(num_episodes):
        obs = env.reset()
        state = q_learning_agent.discretize_state(obs)
        episode_reward = 0
        done = False
        while not done:
            action = q_learning_agent.select_action(state)
            step_result = env.step(action)
            obs, reward, done = step_result[:3]
            next_state = q_learning_agent.discretize_state(obs)
            episode_reward += reward
        total_reward += episode_reward
    return total_reward / num_episodes